"""AI Workflow module for paper analysis and article generation.

This module implements a multi-phase AI workflow using Prompt-Chaining
and Evaluator-Optimizer patterns:

Phase 1: Paper Analysis (論文読む係のLLM)
Phase 2: Article Generation (記事書く係のLLM)
Phase 3: Evaluation & Improvement Loop (評価用LLM + Evaluator-Optimizer)
"""

import logging

from src.utils.config import load_config
from src.utils.openai import (
    analyze_paper_section,
    evaluate_article,
    generate_article_section,
    revise_article,
)

# Set up logger
logger = logging.getLogger(__name__)

# Load configuration
config = load_config(config_path="config.yaml", logger=logger)


def evaluator_optimizer_loop(
    pdf_images: list, initial_article: str, analysis: str, section: str, max_iterations: int | None = None
) -> str:
    """Execute Evaluator-Optimizer workflow loop.

    This function implements the evaluation-improvement cycle:
    1. Evaluate current article
    2. If pass: return article
    3. If fail: get feedback, revise article, repeat

    Args:
        pdf_images (list): List of base64-encoded PDF page images
        initial_article (str): Initial article generated by article writer
        analysis (str): Analysis result from paper analyzer (for reference)
        section (str): Section name being processed (for logging)
        max_iterations (int, optional): Maximum number of evaluation cycles.
                                       If None, uses config value.

    Returns:
        str: Final article (either passed evaluation or reached max iterations)
    """
    if max_iterations is None:
        workflow_config = config.get("workflow", {})
        evaluator_config = workflow_config.get("evaluator", {})
        max_iterations = evaluator_config.get("max_iterations", 3)
    
    # Get early exit threshold from config
    workflow_config = config.get("workflow", {})
    evaluator_config = workflow_config.get("evaluator", {})
    early_exit_threshold = evaluator_config.get("early_exit_threshold", 8)

    current_article = initial_article
    iteration = 0

    logger.info("Starting Evaluator-Optimizer loop (max iterations: %d)", max_iterations)

    while iteration < max_iterations:
        iteration += 1
        logger.info("Evaluation iteration %d/%d", iteration, max_iterations)

        # Phase 3a: Evaluate article
        evaluation = evaluate_article(pdf_images, current_article, analysis, section=section, iteration=iteration)

        # Check if article passes evaluation
        if evaluation.pass_evaluation:
            logger.info("Article passed evaluation on iteration %d", iteration)
            logger.info("Scores: accuracy=%d, readability=%d, completeness=%d, style=%d, compliance=%d",
                       evaluation.score.accuracy, evaluation.score.readability, evaluation.score.completeness,
                       evaluation.score.style, evaluation.score.compliance)
            return current_article
        
        # Check for early exit based on score threshold
        min_score = min(
            evaluation.score.accuracy,
            evaluation.score.readability,
            evaluation.score.completeness,
            evaluation.score.style,
            evaluation.score.compliance
        )
        if min_score >= early_exit_threshold:
            logger.info("Early exit: All scores >= %d (iteration %d)", early_exit_threshold, iteration)
            logger.info("Scores: accuracy=%d, readability=%d, completeness=%d, style=%d, compliance=%d",
                       evaluation.score.accuracy, evaluation.score.readability, evaluation.score.completeness,
                       evaluation.score.style, evaluation.score.compliance)
            return current_article

        # Article failed evaluation - get feedback and revise
        feedback = evaluation.feedback
        logger.info("Article failed evaluation. Feedback: %s", feedback)
        logger.info("Scores: accuracy=%d, readability=%d, completeness=%d, style=%d, compliance=%d",
                   evaluation.score.accuracy, evaluation.score.readability, evaluation.score.completeness,
                   evaluation.score.style, evaluation.score.compliance)

        # Phase 3b: Revise article based on feedback
        current_article = revise_article(pdf_images, current_article, feedback, analysis, section=section, iteration=iteration)
        logger.info("Article revised based on feedback")

    # Max iterations reached
    logger.warning("Max iterations (%d) reached without passing evaluation", max_iterations)
    logger.info("Returning final version of article")
    return current_article


def process_section_with_workflow(pdf_images: list, section: str, context: str = "") -> tuple:
    """Process a single section using the complete AI workflow.

    This function implements the full Prompt-Chaining workflow:
    Phase 1: Paper Analysis - Extract important information from paper
    Phase 2: Article Generation - Generate article based on analysis
    Phase 3: Evaluation & Improvement - Evaluator-Optimizer loop

    Args:
        pdf_images (list): List of base64-encoded PDF page images
        section (str): Section name to process
        context (str, optional): Context from previous sections. Defaults to "".

    Returns:
        tuple: (analysis_result, final_article)
               - analysis_result: Extracted information from paper
               - final_article: Final article text (passed evaluation or max iterations)
    """
    logger.info("=" * 60)
    logger.info("Processing section with AI workflow: %s", section)
    logger.info("=" * 60)

    # Phase 1: Paper Analysis (論文読む係のLLM)
    logger.info("Phase 1: Paper Analysis")
    analysis = analyze_paper_section(pdf_images, section)
    logger.info("Paper analysis completed for section: %s", section)

    # Phase 2: Article Generation (記事書く係のLLM)
    logger.info("Phase 2: Article Generation")
    initial_article = generate_article_section(pdf_images, section, analysis, context)
    logger.info("Initial article generated for section: %s", section)

    # Phase 3: Evaluation & Improvement (評価用LLM + Evaluator-Optimizer)
    logger.info("Phase 3: Evaluation & Improvement Loop")
    final_article = evaluator_optimizer_loop(pdf_images, initial_article, analysis, section=section)
    logger.info("Final article ready for section: %s", section)

    logger.info("=" * 60)
    logger.info("Workflow completed for section: %s", section)
    logger.info("=" * 60)

    return analysis, final_article


def generate_detailed_summary_with_workflow(pdf_images: list, sections: list) -> str:
    """Generate detailed summary for all sections using AI workflow.

    This is the main entry point for generating detailed summaries.
    It processes each section sequentially using the workflow and
    chains the context forward.

    Args:
        pdf_images (list): List of base64-encoded PDF page images
        sections (list): List of section names to process

    Returns:
        str: Complete detailed summary with all sections
    """
    logger.info("Starting detailed summary generation with AI workflow")
    logger.info("Sections to process: %s", sections)

    detailed_summary = ""
    context = ""

    for section in sections:
        # Process section with workflow
        _, article = process_section_with_workflow(pdf_images, section, context)

        # Add to detailed summary
        detailed_summary += f"\n\n## {section}\n\n{article}"

        # Update context for next section (Prompt-Chaining)
        context = detailed_summary

        logger.info("Section '%s' added to detailed summary", section)

    logger.info("Detailed summary generation completed")
    return detailed_summary
